---
title: ASR tutorial
author: Joe Pearce
date: 2024-09-12
category: Jekyll
layout: post
---

# Terminology
Come back to this part of the tutorial if you see terminology that doesn't make sense to you. 
Please note, I am not an expert in any of this, but a mere sociolinguist/phonetician who has learnt about ASR in order to use it. These explanations come from my own understanding, so feel free to correct me if you disagree with my explanations.


| Term     | Explanation |
| :------- | :------- |
|**ASR**| Automatic Speech Recognition. Refers to using automatic methods to transcribe speech with minimal human input. Confusingly, ASR can sometimes also stand for automatic *speaker* recognition, but I won't be using it to mean that here.|
|**STT**| Speech-to-Text - Another way of saying ASR. |
|**GPU** | Graphics Processing Unit - Part of a computer or graphics card that allows you to separate a task like ASR into parallel tasks which can be run at the same time, speeding up the process. Better known for allowing you play video games. |
|**Model**| The statistics and probability that enables ASR to function. |  
|**VRAM**| Virtual Random Access Memory: A resource often required to run large ASR models, which can be increased by using a GPU. |
|**AI**| Artificial Intelligence - An application with the ability to learn information in order to complete a task. |
|**Training data**| For ASR, paired speech and transcripts that are fed to the model, allowing it to learn the relationship between text and audio. Training data is often subtitle data scraped from the internet. |
|**Prompt**| An instruction you give to an AI model, that can help shape the output it gives you.|
|**VAD**| Voice Activity Detection - Separation of human speech from silence, background noise, music, and so on. Often the first step in ASR. |
|**High-performance computer cluster**| A set of computers with a lot of processing power that your institution may have access to, allowing you to run ASR on GPU without transferring the data elsewhere.|
|**API**| Application Programming Interface - Part of a program stored on someone else's computer that allows data and services to be sent between your computer and theirs. APIs are used in ASR as the backend of more user-friendly ASR applications, which don't require you to download the ASR model, but instead let you send your data to a cloud-based implementation of it, request for it to be transcribed, and receive a transcribed file in return.|
|**GUI**| Graphical User Interface - An application where you click buttons to make things happen, rather than writing code. |
|**Diarization**| In a multi-speaker audio sample, the answer to who is speaking when.|
|**Command line**| A text-based interface for interacting with the files on your computer. You might for example, use it to tell your ASR to run on a particular folder. Introduction to it [here](https://tutorial.djangogirls.org/en/intro_to_command_line/). |

# What tools are out there?

There's a lot of options to choose from when it comes to ASR, and which one is best for you for will depend on a couple of factors:

-How much time you're willing to invest in learning how to run it 
-What kind of data you have (i.e. number of speakers, channels)
-Whether you have access to a high-performance computer cluster or GPU through your university
-If you, your participants and your institution's ethics board and data protection team are willing to transfer your data to someone else's servers during processing
-How much you care about how 'clean' your transcript is
-Whether you need accurate time-stamps and speaker diarization

For the purposes of this guide, I'm going to into more detail on OpenAI's Whisper, because this is what I know the most about, and because it's open source and free to install. It has numerous offshoots, some of which make it easier to process your data for use in sociolinguistic research. I'll briefly talk about some other options below.


## OpenAI Whisper and its children
You've probably heard of [OpenAI](https://openai.com/) as the company behind ChatGPT, DALL-E, and Sora. [Whisper](https://github.com/openai/whisper) is their set of speech recognition models, that allow transcription of audio data in a number of languages. A fun feature of Whisper is that it allows you to specify the language, guess what language it's hearing, or translate what it's hearing into English. You can learn more from the paper they put out about it [here](https://cdn.openai.com/papers/whisper.pdf).

Whisper has a number of models available, for various languages, and of varying sizes. As a rule of thumb, if you want more 'accurate' speech recognition, you have to use 'bigger models', which require more processing power. Smaller models, on the other hand, run quicker, and require less processing power, but come at a tradeoff for accuracy. Larger models use more parameters to describe the data, a concept that I admit I can't really get my head around. 

As the name suggests, OpenAI's models are open: Available for anyone to download and run on their own computer, and use the code to develop other apps. Because of this, you can find a lot of ASR systems which use Whisper as their starting point. Some of these have been collated [here](https://github.com/sindresorhus/awesome-whisper), and some which you might be interested in are:
- [faster-whisper](https://github.com/SYSTRAN/faster-whisper). Like Whisper, but faster.
- [whisper-timestamped](https://github.com/linto-ai/whisper-timestamped). Like Whisper, but with better timestamps.
-[WhisperX](https://github.com/m-bain/whisperX), which improves processing speed, alignment between audio and transcript, and implements speaker diarization (using [faster-whisper](https://github.com/SYSTRAN/faster-whisper), [wav2vec](https://arxiv.org/abs/2006.11477), and [pyannote](https://github.com/pyannote/pyannote-audio). My preferred option for most ASR in sociolinguistics and phonetics.

There are two main ways to run Whisper (or any of its children):
- Locally, by downloading the model onto your own computer and running it via the command line.
  + You can either do this on a CPU (=your computer's normal processor), which will be slower, or a GPU, which will speed things up.
  + Buying your own GPU is expensive, but you might have access to one already via your institution's high-perfomance computer cluster, or because you are a gamer. 
- Using an API, where you send the data to someone else's servers that are running Whisper, and get them to do it for you.

**Why would I run it locally?**
- Cost. Many APIs will charge you when you go over a certain about of data (usually, about five hours of audio), and so by running it locally you're hypothetically saving money. In practice, the cost of running it locally isn't actually free, as you or your institution might need to pay for GPU access, or power to run the GPU, but you're less likely to have to spend your personal funds or grant funds to run it locally.  
- Data privacy and security. By downloading Whisper's models onto your own computer, you spend less time worrying about where it's going and who is doing what with it. 
- Customizability. You have the choice between the original Whisper or any of its children mentioned above, and full control over the settings you use. 
- Time. You don't need to wait for files to upload and download, and with a little effort you can set it to run on a whole folder rather than a single file.

**Why would I use an API?**
- Resources/cost. If you don't have access to a GPU, but you still want to use the larger Whisper models, you can usually use an API for free up until a certain point. 
- User-friendliness. If you aren't interested in learning how to install Whisper and its dependencies, which can be intimidating if you haven't done anything like this before, and might take a while. 
- Not having admin privileges on your device. Some institutions will make it very hard to install anything on managed machines. You usually can install something like Whisper, if you're a staff member or research student, and your university has a good IT team, but you might take some convincing. If you're an undergraduate student or taught postgrad student using a university machine, you'll probably need a staff member to make the request for you.  



## Other options
Some additional tools I've seen mentioned, but which I haven't tested extensively myself:
- [vosk](https://github.com/alphacep/vosk-api) speech recognition toolkit, which you can download and run locally offline
  + [vosk](https://github.com/alphacep/vosk-api) is the backend for [audapolis](https://github.com/bugbakery/audapolis), a GUI which allows you to use download and run vosk ASR models, and then correct the output.
  + vosk has smaller models than many options, so it's often used for developing apps which need something lightweight to run locally - this means it's a good option if you don't have access to a GPU, but also have data privacy concerns about using a cloud-based option. 
- [OpenBrainAI](https://openbrainai.com/downloads/). I don't know anything about this one, but it was recommended to me recently.
- [Speechmatics](https://www.speechmatics.com/). Supposedly very good, but paid-for, API service. 
- [Microsoft Azure Speech-to-Text](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/speech-to-text). A paid-for API service available from Microsoft. 
  + This service is popular in sociolinguistics, and I hear it's very good - But note that this in also an API service and does **not** run locally. [More information is available here](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription-audio-data?tabs=portal): your data needs to be temporarily stored on either in Azure Blob storage or on a publicly accessible URL in order to be transcribed. 
  -Microsoft Azure's service is the backend for [CLOx](https://clox.ling.washington.edu/#/), another popular option for linguists.
- [Deepgram](https://deepgram.com/). Again, a paid service which requires transferring your data to their servers. Privacy notice available [here](https://deepgram.com/privacy/).
  + Deepgram is the backend for DARLA's [bedword](http://darla.dartmouth.edu/bedword) service, which is a great option if you want to integrate it with automated vowel extraction. 

Paid options mentioned above are worth looking into if you're working with small amounts of data, as many offer a small amount of transcription for free (often up to about 5 hours a month). Some people may even have access to Microsoft Azure via their institutional login. And many of the paid options are more intuitive to learn and use than free options. It's also possible some of those options may perform better than free options, though I haven't looked into it. Diarization is one area where I suspect paid models might be useful, 

## When NOT to use ASR

ASR is not a solution to all of the world's transcription problems. There are a few cases where I would advise against using ASR, specifically:

- You're working with data that includes speakers with non-standard accents, and a test run indicates that the ASR struggles with the accent in question
  + As an example, this is the case on our project My Voice, My Glasgow (with Sadie Ryan and Emma Moore), where we are working with young people in Glasgow, many of whom have Glaswegian accents, use Scots, and have a different home language that influences their accent when speaking Scots/English. I was actually impressed with how well ASR coped with some of this, but it was still nowhere near good enough to be time-saving rather than time-wasting, compared to transcribing manually.
- You have a specific set of transcription guidelines that you'd like to follow, and implementing these after ASR is conducted won't save time compared to transcribing from scratch.
- You're doing in-depth qualitative research where transcription is seen as part of the process of familiarizing yourself with the data (e.g. Interpretative Phenomenological analysis).
- You don't have access to the resources you need (e.g. a GPU) and have concerns about using a cloud-based service. 


# WhisperX tutorial 

## Local installation
For this section, I'll give guidance for installing on Windows 10. If you are working on a University computer or your institution has access to a high-perfomance computer cluster, your IT department should help you set this up.

Please note that the instructions below are just what I personally did, and what worked for me and my machine.

### Step One: Install conda
conda is a package and environment management system that is useful for installing and running WhisperX. It allows you to download nearly all of the dependencies (= other packages you need to install for WhisperX to run) at the same time as installing WhisperX. You can choose whether you want to install the full version of Anaconda or just Miniconda, a smaller version of it, but for this I'd recommend Miniconda, because it does what we need and it takes up less space on your computer. You can get it [here](https://docs.anaconda.com/miniconda/miniconda-install/).

If you aren't able to install it on your institution's managed computers, get support from them - You might need to ask for special administrative rights.

### Step Two: Set up a conda environment and install dependencies
One of the reasons conda is useful is because you might have different pieces of software on your computer that need different versions of different dependencies - conda lets you give each of these a separate spot on your computer, called an environment, where these different versions can live without getting in each other's way. We're going to set one of these up for WhisperX. 

Assuming you installed Miniconda above, you should now be able to use it's own command prompt. Look for something called Anaconda Prompt (miniconda3) and run it. A black window should pop up. Run the following code to create an environment to install WhisperX in:

```
conda create --name whisperx python=3.10
```

This essentially says, `Conda, please create a virtual environment for me, name it whisperx, and install Python v3.10 inside it'. You might already have Python installed somewhere else on your computer, but what's useful about this is that we can install v3.10 just in this part of you computer, without upsetting anything else you're doing with a different version of Python. Other versions of Python might work just fine, but v3.10 is just what the developer has tested it on. You can call it something else if you like by changing 'whisperx' to whatever you'd like to call it.

If you are using a device managed by your University, you might additionally have to specify where you'd like this virtual environment to be created. My laptop really wanted to put it in my roaming user profile, which caused problems later down the line. If this is the case, you should alternatively run something like this to specify where you'd like to create the environment. 

```
conda create --prefix C:\Users\youruser\.conda\envs\whisperx python=3.10
```
You'll get a little prompt showing you what packages it will install, and asking if you want to proceed. Type 'y' and press enter to agree.

You can then activate this environment by running this:

```
conda activate whisperx
``` 

You can tell whether you are 'in' your WhisperX environment or not as it will display the name of your environment in brackets before the directory. So it will look something like this

  Now that the environment has been activated, anything you install will just install here. So we can go ahead an install the dependencies like this:

```
conda install pytorch==2.0.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia
```

Wait a bit, grab some coffee. These are machine learning packages, one of which (torchaudio) is specifically for working with audio files and doing signal processing.


I also installed [ffmpeg](https://www.ffmpeg.org/) using [scoop](https://scoop.sh/), following [the instructions given on the Whisper installation page](https://github.com/openai/whisper). I've since learned you maybe can actually get this through conda, but anyway, I've never done that so I'll stick to recommending doing it via scoop. ffmpeg is actually a great tool to have installed if you're working with sound files, as it lets you convert between formats.


```
scoop install ffmpeg
```


### Step Three: Install WhisperX
You've made it! Now you can install what you came here for:

```
pip install git+https://github.com/m-bain/whisperx.git
```

## Some troubleshooting
If, as described above, you've faced a similar issue where things are installing in unexpected places, you can specify the installation location like this:

```
pip install --target C:\Users\youruser\.conda\envs\whisperx git+https://github.com/m-bain/whisperx.git
```

If you've run into an error to do with NumPy, downgrade the Numpy version to less than v. 2.0.0. Here's what I did:

```
conda install -c conda-forge numpy=1.26.4
```

## Running a test file
Before we start fiddling with different settings, you're going to want to check it's working by running WhisperX on a short file. For the purposes of this tutorial, we'll use the file provided on the WhisperX GitHub.

I'd recommend using a recording from [the Speech Accent Archive](https://accent.gmu.edu), because they're usually quite short so won't take long to run - so you won't be left for an hour wondering whether anything is actually happening. I'll just use the recording [english1.mp3](https://accent.gmu.edu/browse_language.php?function=detail&speakerid=61), which is a male speaker from Pittsburgh reading 'Please call Stella'. 

Because it's a test file, we'll run it using the CPU - i.e. using your computer's normal processor. We'll also use the tiny model, which uses the least amount of processing power, and tell it to expect to find English. This should mean it's possible to run this code on most computers. It might not give the best output, but for now, we just want to see if it's working. 


In the Anaconda Prompt window with your whisperx environment activated, navigate to wherever your file is stored by using the 'cd' command to change directory, like this:

```
cd C:\Users\Joe\my_whisperx_files\
```

You'll now be in the folder where you stored your test file. Run it through WhisperX on the most basic settings like this:

```
whisperx english1.mp3 --language en --model tiny --compute_type int8
```

This says: run WhisperX on this file, expect English, use the tiny model, and use the CPU resources instead of looking for a GPU. 
You'll see this running, and you might see it downloading the model along the way. It will give you a few warnings about the torchaudio backend being deprecated, which you can ignore. 

Pretty quickly, some new files will appear with your transcript. These are:

- A .txt file, containing *only* the text of your transcript in plain text.
- .tsv, .srt, and .vtt formats, which are common subtitle formats. These should contain timestamps.
- A .json file, which contains the start and end time of each chunk transcribed, followed by the transcript of the chunk, and then within that:
  + The start time and end time of each word in the chunk
  + The confidence score. To be honest, I'm unclear on whether this is the confidence on the accuracy of the word, the timestamps, or some combination of these. 

### Specifying an output directory

By default, the output will appear in whatever you working directory is - wherever you navigated to above when you changed directory. This can be a bit impractical, but luckily WhisperX lets you specify the output with the handy 'output_dir' command. 


```
whisperx english1.mp3 --language en --model tiny --compute_type int8 --output_dir path\to\your_folder
```

## Running WhisperX on GPU
If you want to run WhisperX on longer files, use larger models, implement speaker diarization, or run a large number of files, you might want to consider running it on a GPU instead. 

If you don't know whether your computer has a GPU, you can check by [following these instructions](https://www.laptopmag.com/articles/check-vram-windows-10). The 'dedicated video memory' that you find by following that guide is the amount of VRAM you have. [The OpenAI Whisper GitHub](https://github.com/openai/whisper) has a breakdown of how much VRAM you need to run different size models. You might have an graphics card that means that you have enough VRAM to use the GPU on your computer, but I did not, so can't give any guidance on setting that up. My own laptop has 128MB, which is much less that the 1GB they recommend for running the tiny model! 

Because of this, I set it up on the University of York's Viking cluster. If you know that you need to run WhisperX on a lot of data and are based at a university, it's worth looking into whether there are any options for this. If they have something similar, it is likely to run on Linux, and you might have to go through slightly different steps to set it up. At York, I followed the following steps:

1. Got in touch with IT to set up an account.
2. Connect to the University VPN.
3. Set up WinSCP to transfer files to Viking.
3. ssh onto Viking.
4. Load miniconda.
5. Create and activate a WhisperX virtual environment.
6. Install dependencies.
7. Install WhisperX.
8. Use jobscripts to submit WhisperX jobs. 

A lot of this is quite similar to setting it up on Windows, but with some different syntax and a few extra steps. My impression is that similar systems at different universities are unlikely to work in the exact same way, so I won't go into details here - instead, get support from your IT team.

Once you have it set up, it's just the same to run it, except you don't have to tell it to use the GPU, because this is the default setting. For example, you could run: 

```
whisperx english1.mp3 --language en --model tiny
```

This would run the same as our example above on CPU, but on the GPU. In what follows, I'll assume that you know whether you're using CPU or GPU, and know what steps to follow.

## Running WhisperX on multiple files
If you know the names of all your files, you can do this by simply listing the names of all your files in the command, e.g.:

```
whisperx my_folder/english1.mp3 my_folder/english2.mp3 --language en --model tiny --compute_type int8
```

Sometimes, though, it might be more useful to put all of files into a particular folder and tell it to run on all the files in that folder. You can do this using a FOR loop, and then it will iterate through each file in your directory. 

```
FOR %i IN (my_folder\*.mp3) DO whisperx --language en --model tiny --compute_type int8 --output_dir my_folder %i
```

You can tell it to look for all files with a specific extension - here I've told it to look for .mp3 files, but you could do the same for .wav, or any file type that WhisperX supports. It will then find each .mp3 in 'my_folder' and run the command on each file in turn. 

## What else can WhisperX do?

WhisperX comes with a whole host of different fun options. To see what they all are, run this:

```
whisperx --help
```

This will list out all the options available. For now though, we'll focus on of its more interesting/useful capabilities.


## Diarization

*To come...*

## Converting WhisperX output to other formats

*To come...*

## Prompting

*In progress...*

## Using a larger alignment model

*Still under construction...*

### Troubleshooting

If you're having trouble with WhisperX, try playing around with these settings.

**--batchsize** Reducing this will reduce the GPU memory required. The default is 8, but try, e.g. '--batch_size 4' if you have a small GPU and see if it helps.
**--vad_onset** This is a Voice Activity Detection Setting. The default is 0.5. If you find speech isn't being detected, try reducing it. To be honest, I haven't found this helpful.
**--vad_offset** This is a Voice Activity Detection Setting. The default is 0.363. If you find speech isn't being detected, try reducing it. To be honest, I haven't found this helpful.
**--suppress_tokens** This setting lets you suppress special characters. Its default setting will suppress most special characters except punctuation, but if you're having trouble with it inserting special characters into your transcription, you can add these here in a comma-separated list. 
**--no_speech_threshold** Another one to experiment with if speech isn't being detected. The default is 0.6 - increasing it, should, theoretically, cause it to find more speech in quiet sections and background noise. The tradeoff is that it may hallucinate more.
